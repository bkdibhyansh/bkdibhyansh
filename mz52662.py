# -*- coding: utf-8 -*-
"""MZ52662.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4CZ7LrjsyiZa54ftiB5RXKgMoII0p1G
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    log_loss
)

pd.set_option("display.max_columns", None)
sns.set(style="whitegrid")

df=pd.read_csv('/content/SILKYSKY_DATA_CW1 (S).csv', encoding='latin1')
df.head(5)

print("Dataset shape:", df.shape)
df.info()

missing = df.isna().sum()
print("Missing values:\n", missing[missing > 0])

print("Duplicate rows:", df.duplicated().sum())

df['Arrival Delay in Minutes'] = df['Arrival Delay in Minutes'].fillna(
    df['Arrival Delay in Minutes'].median()
)

df['Satisfied'] = df['Satisfied'].map({'Y': 1, 'N': 0})

df_model = df.drop(columns=['Ref', 'id'])

categorical_cols = df_model.select_dtypes(include='object').columns

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_model[col] = le.fit_transform(df_model[col])
    label_encoders[col] = le

sns.countplot(x='Satisfied', data=df)
plt.title("Passenger Satisfaction Distribution")
plt.show()

plt.figure(figsize=(14,10))
sns.heatmap(df_model.corr(), cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap of Passenger Features")
plt.show()

sns.boxplot(x='Satisfied', y='Departure Delay in Minutes', data=df)
plt.title("Departure Delay vs Satisfaction")
plt.show()

X = df_model.drop(columns=['Satisfied'])
y = df_model['Satisfied']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train_scaled, y_train)

y_pred_log = log_model.predict(X_test_scaled)
y_prob_log = log_model.predict_proba(X_test_scaled)

print("Logistic Regression Performance")
print(classification_report(y_test, y_pred_log))
print("Log Loss:", log_loss(y_test, y_prob_log))

cm_log = confusion_matrix(y_test, y_pred_log)
sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix – Logistic Regression")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)

print("Random Forest Performance")
print(classification_report(y_test, y_pred_rf))
print("Log Loss:", log_loss(y_test, y_prob_rf))

cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix – Random Forest")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

feature_importance = pd.Series(
    rf_model.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

plt.figure(figsize=(10,6))
feature_importance.head(10).plot(kind='bar')
plt.title("Top 10 Feature Importances – Random Forest")
plt.ylabel("Importance Score")
plt.show()

comparison = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_log),
        accuracy_score(y_test, y_pred_rf)
    ],
    "Precision": [
        precision_score(y_test, y_pred_log),
        precision_score(y_test, y_pred_rf)
    ],
    "Recall": [
        recall_score(y_test, y_pred_log),
        recall_score(y_test, y_pred_rf)
    ],
    "F1 Score": [
        f1_score(y_test, y_pred_log),
        f1_score(y_test, y_pred_rf)
    ]
})

comparison


metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
log_scores = comparison.loc[0, metrics]
rf_scores = comparison.loc[1, metrics]

x = np.arange(len(metrics))
width = 0.35

plt.figure(figsize=(10,6))

plt.bar(x - width/2, log_scores, width, label='Logistic Regression')
plt.bar(x + width/2, rf_scores, width, label='Random Forest')

plt.xticks(x, metrics)
plt.ylim(0.85, 1.0)
plt.ylabel('Score')
plt.title('Model Performance Comparison Across Evaluation Metrics')
plt.legend()

plt.show()

mock_passenger = X_test.iloc[[0]].copy()

rf_prediction = rf_model.predict(mock_passenger)
rf_probability = rf_model.predict_proba(mock_passenger)

print("Predicted Satisfaction (1=Yes, 0=No):", rf_prediction[0])
print("Prediction Probabilities [Unsatisfied, Satisfied]:", rf_probability[0])

